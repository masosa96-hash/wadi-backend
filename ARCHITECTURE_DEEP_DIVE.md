# ğŸ§  ARQUITECTURA PROFUNDA - WADI BRAIN

## ğŸ“ DiseÃ±o del Sistema

### Cerebro Dual (Kivo + Wadi)

WADI utiliza una arquitectura de **doble procesamiento** inspirada en la cogniciÃ³n humana:

```
USER INPUT
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   KIVO (Reasoning)    â”‚ â† "Sistema 2" (Pensamiento deliberado)
â”‚   - Analiza intenciÃ³n â”‚
â”‚   - Crea plan         â”‚
â”‚   - EvalÃºa contexto   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ KivoThought
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WADI (Execution)    â”‚ â† "Sistema 1" (AcciÃ³n rÃ¡pida)
â”‚   - Ejecuta plan      â”‚
â”‚   - Llama tools       â”‚
â”‚   - Genera respuesta  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ WadiAction
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpenAI API          â”‚ â† GeneraciÃ³n de lenguaje natural
â”‚   - GPT-3.5-turbo     â”‚
â”‚   - Conversacional    â”‚
â”‚   - Contextual        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
USER RESPONSE
```

---

## ğŸ” KIVO: El Motor de Razonamiento

### Responsabilidades:

1. **AnÃ¡lisis de IntenciÃ³n**: Clasifica el tipo de solicitud
2. **PlanificaciÃ³n**: Crea pasos para cumplir la solicitud
3. **Contexto**: Determina quÃ© informaciÃ³n adicional se necesita
4. **Confianza**: EvalÃºa quÃ© tan seguro estÃ¡ del anÃ¡lisis

### Tipos de Intent:

```typescript
type Intent =
  | "chat" // ConversaciÃ³n general
  | "command" // Comando directo (crear, eliminar, etc)
  | "query" // BÃºsqueda de informaciÃ³n
  | "creation"; // CreaciÃ³n de recursos (proyecto, archivo, etc)
```

### ImplementaciÃ³n Actual:

```typescript
// apps/api/src/services/brain/kivo.ts
export async function pensar(
  input: string,
  context: any = {},
): Promise<KivoThought> {
  const normalizedInput = input.toLowerCase().trim();

  // 1. AnÃ¡lisis de palabras clave (heurÃ­stica simple)
  let intent: Intent = "chat";
  const reasoning: string[] = [];
  const plan: string[] = [];

  // 2. DetecciÃ³n de patrones
  if (includes("crear", "nuevo")) {
    intent = "creation";
    reasoning.push("Usuario quiere crear algo");
    plan.push("Identificar tipo de recurso");
    plan.push("Pedir detalles faltantes");
    plan.push("Ejecutar creaciÃ³n");
  }
  // ... mÃ¡s patrones

  // 3. Retornar pensamiento estructurado
  return {
    intent,
    confidence: 0.8,
    reasoning,
    plan,
    context_needed: [],
  };
}
```

### ğŸš€ Mejora Futura (LLM-based):

```typescript
// VersiÃ³n avanzada con OpenAI para anÃ¡lisis
async function pensarConLLM(input: string, context: any) {
  const analysisPrompt = `
  Analiza la siguiente solicitud del usuario y determina:
  1. IntenciÃ³n principal (chat, command, query, creation)
  2. Nivel de confianza (0-1)
  3. Razonamiento (lista de puntos)
  4. Plan de acciÃ³n (pasos)
  5. Contexto necesario
  
  Usuario: "${input}"
  Contexto: ${JSON.stringify(context)}
  
  Responde en JSON con estructura:
  {
    "intent": "...",
    "confidence": 0.0,
    "reasoning": ["...", "..."],
    "plan": ["...", "..."],
    "context_needed": ["...", "..."]
  }
  `;

  const response = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [
      {
        role: "system",
        content: "Eres un asistente de anÃ¡lisis de intenciÃ³n.",
      },
      { role: "user", content: analysisPrompt },
    ],
    temperature: 0.3, // Baja para anÃ¡lisis consistente
  });

  return JSON.parse(response.choices[0].message.content);
}
```

---

## âš¡ WADI: El Motor de EjecuciÃ³n

### Responsabilidades:

1. **InterpretaciÃ³n**: Lee el pensamiento de Kivo
2. **EjecuciÃ³n**: Llama a las herramientas necesarias
3. **GeneraciÃ³n**: Crea la respuesta final
4. **ValidaciÃ³n**: Verifica que la acciÃ³n fue exitosa

### Tipos de AcciÃ³n:

```typescript
type ActionType =
  | "response" // Respuesta directa de texto
  | "tool_call" // Llamada a una herramienta externa
  | "error"; // Error en la ejecuciÃ³n
```

### ImplementaciÃ³n Actual:

```typescript
// apps/api/src/services/brain/wadi.ts
export async function ejecutar(
  thought: KivoThought,
  context: any = {},
): Promise<WadiAction> {
  switch (thought.intent) {
    case "creation":
      return {
        type: "tool_call",
        payload: {
          tool: "create_resource",
          params: extractParams(context),
        },
        thought_process: thought,
      };

    case "query":
      return {
        type: "tool_call",
        payload: {
          tool: "search",
          query: extractQuery(context),
        },
        thought_process: thought,
      };

    case "chat":
    default:
      // Para chat, delega a generateChatCompletion
      return {
        type: "response",
        payload: {
          text: await generateAIResponse(context),
        },
        thought_process: thought,
      };
  }
}
```

### ğŸš€ Mejora Futura (Tool Registry):

```typescript
// Sistema de herramientas registradas
interface Tool {
  name: string;
  description: string;
  parameters: ParameterSchema;
  execute: (params: any) => Promise<any>;
}

class ToolRegistry {
  private tools = new Map<string, Tool>();

  register(tool: Tool) {
    this.tools.set(tool.name, tool);
  }

  async execute(toolName: string, params: any) {
    const tool = this.tools.get(toolName);
    if (!tool) throw new Error(`Tool ${toolName} not found`);

    // Validar parÃ¡metros
    this.validateParams(params, tool.parameters);

    // Ejecutar
    return await tool.execute(params);
  }
}

// Uso en Wadi:
async function ejecutarConTools(thought: KivoThought) {
  if (thought.intent === "creation") {
    const result = await toolRegistry.execute("create_project", {
      name: extractedName,
      type: extractedType,
    });

    return {
      type: "tool_call",
      payload: result,
      thought_process: thought,
    };
  }
}
```

---

## ğŸ¯ Flujo Completo de un Mensaje

### Paso a Paso:

```
1. USER: "Hola WADI, Â¿cÃ³mo estÃ¡s?"
   â”‚
   â†“
2. FRONTEND: chatStore.sendMessage()
   â”‚
   â”œâ”€ Optimistic update (agrega mensaje al UI)
   â”œâ”€ Obtiene historial completo de localStorage
   â””â”€ POST /api/chat
      Headers: { x-guest-id: "uuid" }
      Body: {
        message: "Hola WADI, Â¿cÃ³mo estÃ¡s?",
        messages: [
          { role: "user", content: "Mensaje anterior..." },
          { role: "assistant", content: "Respuesta anterior..." }
        ]
      }
   â”‚
   â†“
3. BACKEND: authMiddleware
   â”‚
   â”œâ”€ Detecta guest_id en header
   â”œâ”€ GUEST_MODE=true â†’ permite paso
   â””â”€ req.user_id = undefined (es guest)
   â”‚
   â†“
4. BACKEND: chatController.sendMessage()
   â”‚
   â”œâ”€ Detecta !userId && guestId â†’ modo guest
   â”œâ”€ Prepara mensajes para OpenAI:
   â”‚  [
   â”‚    { role: "system", content: "Sos WADI..." },
   â”‚    ...historial previo,
   â”‚    { role: "user", content: "Hola WADI, Â¿cÃ³mo estÃ¡s?" }
   â”‚  ]
   â”‚
   â†“
5. BRAIN: pensar() (Kivo)
   â”‚
   â”œâ”€ Analiza: "Hola WADI, Â¿cÃ³mo estÃ¡s?"
   â”œâ”€ Intent: "chat" (no detecta keywords especiales)
   â”œâ”€ Confidence: 0.8
   â”œâ”€ Reasoning: ["Standard conversation detected."]
   â””â”€ Plan: ["Analyze sentiment", "Generate helpful response"]
   â”‚
   â†“
6. BRAIN: ejecutar() (Wadi)
   â”‚
   â”œâ”€ Lee thought.intent === "chat"
   â”œâ”€ DecisiÃ³n: Usar OpenAI directamente
   â””â”€ Llama generateChatCompletion(messages)
   â”‚
   â†“
7. OPENAI: generateChatCompletion()
   â”‚
   â”œâ”€ Model: "gpt-3.5-turbo"
   â”œâ”€ Max tokens: 1000
   â”œâ”€ Temperature: 0.7 (conversacional)
   â”œâ”€ Messages: [system, ...history, user]
   â”‚
   â†“  [API CALL]
   â”‚
   â† "Â¡Hola! Estoy muy bien, gracias por preguntar. Â¿CÃ³mo puedo ayudarte hoy?"
   â”‚
   â†“
8. BACKEND: Retorna respuesta
   â”‚
   Response: {
     ok: true,
     data: {
       reply: "Â¡Hola! Estoy muy bien...",
       assistantMessage: {
         role: "assistant",
         content: "Â¡Hola! Estoy muy bien...",
         created_at: "2025-11-23T15:30:00Z"
       },
       thought: { intent: "chat", confidence: 0.8, ... }
     }
   }
   â”‚
   â†“
9. FRONTEND: Recibe respuesta
   â”‚
   â”œâ”€ Agrega assistantMessage al state
   â”œâ”€ Guarda en localStorage:
   â”‚  localStorage.setItem(`wadi_conv_${guestId}`, JSON.stringify([
   â”‚    ...previousMessages,
   â”‚    userMessage,
   â”‚    assistantMessage
   â”‚  ]))
   â””â”€ UI actualiza automÃ¡ticamente (React state)
   â”‚
   â†“
10. USER: Ve la respuesta en pantalla
```

---

## ğŸ”§ Optimizaciones Implementadas

### 1. **Persistencia Eficiente**

```typescript
// En chatStore.ts
const updatedMessages = [...state.messages, assistantMessage];

// Guarda solo si es guest
if (guestId) {
  localStorage.setItem(`wadi_conv_${guestId}`, JSON.stringify(updatedMessages));
}
```

**Por quÃ© es eficiente:**

- Solo guarda cuando hay cambios
- Serializa solo los datos necesarios
- No hace round-trips a servidor

### 2. **Optimistic Updates**

```typescript
// Antes de enviar al servidor
set((state) => ({
  messages: [...state.messages, newUserMessage],
}));

// Usuario ve el mensaje inmediatamente
// No espera respuesta del servidor
```

**Beneficio:**

- UI instantÃ¡nea
- Mejor experiencia de usuario
- Reduce sensaciÃ³n de lag

### 3. **Context Windowing**

```typescript
// Solo Ãºltimos 10 mensajes para contexto
const { data: history } = await supabase
  .from("messages")
  .select("role, content")
  .eq("conversation_id", currentConversationId)
  .order("created_at", { ascending: true })
  .limit(10); // â† LIMITADO
```

**Por quÃ©:**

- Reduce tokens enviados a OpenAI
- Mantiene costos bajos
- OpenAI tiene lÃ­mite de contexto (4096 tokens para gpt-3.5-turbo)

### 4. **Error Handling Robusto**

```typescript
try {
  const response = await api.post("/api/chat", {...});
  // ...
} catch (error: any) {
  console.error("Error sending guest message:", error);
  set({ error: "Error al enviar mensaje", sendingMessage: false });
}
```

**Cobertura:**

- Network errors
- API errors (405, 422, 500)
- OpenAI rate limits
- Validation errors

---

## ğŸ“Š MÃ©tricas y Monitoreo

### Logs Estructurados:

```typescript
// En chatController.ts
console.log(
  "[sendMessage] Request from:",
  userId ? `User ${userId}` : `Guest ${guestId}`,
  { message: message?.substring(0, 50), conversationId },
);

console.log("[sendMessage] Kivo thought:", thought);
console.log("[sendMessage] Calling OpenAI with", messages.length, "messages");
```

### QuÃ© monitorear:

1. **Tiempo de respuesta**:

   ```typescript
   const start = Date.now();
   const response = await generateChatCompletion(messages);
   const duration = Date.now() - start;
   console.log(`[Perf] OpenAI responded in ${duration}ms`);
   ```

2. **Tasa de error**:

   ```typescript
   let errorCount = 0;
   let totalRequests = 0;

   try {
     totalRequests++;
     await sendMessage();
   } catch {
     errorCount++;
   }

   console.log(
     `Error rate: ${((errorCount / totalRequests) * 100).toFixed(2)}%`,
   );
   ```

3. **Uso de tokens**:
   ```typescript
   const completion = await openai.chat.completions.create({...});
   const tokensUsed = completion.usage?.total_tokens || 0;
   console.log(`[OpenAI] Used ${tokensUsed} tokens`);
   ```

---

## ğŸš€ Optimizaciones Futuras

### 1. **Streaming Responses**

```typescript
// En lugar de esperar toda la respuesta:
export async function* streamChatCompletion(messages) {
  const stream = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages,
    stream: true, // â† STREAMING
  });

  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content;
    if (content) {
      yield content; // EnvÃ­a palabra por palabra
    }
  }
}
```

**Beneficio:**

- Usuario ve respuesta aparecer en tiempo real
- SensaciÃ³n de "pensando" mÃ¡s natural
- Mejor UX

### 2. **Caching de Respuestas**

```typescript
// Cache para preguntas comunes
const responseCache = new Map<string, string>();

async function getCachedOrGenerate(input: string) {
  const normalized = input.toLowerCase().trim();

  if (responseCache.has(normalized)) {
    return responseCache.get(normalized)!;
  }

  const response = await generateChatCompletion([...]);
  responseCache.set(normalized, response);

  return response;
}
```

**Casos de uso:**

- "Hola" â†’ Siempre similar
- "Â¿QuÃ© puedes hacer?" â†’ FAQ
- "Ayuda" â†’ GuÃ­a

### 3. **Embeddings para Contexto SemÃ¡ntico**

```typescript
// Buscar mensajes relevantes del historial
async function getRelevantContext(query: string, allMessages) {
  // 1. Generar embedding del query
  const queryEmbedding = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });

  // 2. Calcular similitud con historial
  const scored = allMessages.map((msg) => ({
    ...msg,
    similarity: cosineSimilarity(
      queryEmbedding.data[0].embedding,
      msg.embedding,
    ),
  }));

  // 3. Tomar top 5 mÃ¡s relevantes
  return scored.sort((a, b) => b.similarity - a.similarity).slice(0, 5);
}
```

### 4. **Rate Limiting Inteligente**

```typescript
// LÃ­mite por usuario
const userRateLimits = new Map<
  string,
  {
    count: number;
    resetAt: number;
  }
>();

function checkRateLimit(userId: string) {
  const limit = userRateLimits.get(userId);
  const now = Date.now();

  if (!limit || now > limit.resetAt) {
    userRateLimits.set(userId, {
      count: 1,
      resetAt: now + 60000, // 1 minuto
    });
    return true;
  }

  if (limit.count >= 10) {
    // Max 10 mensajes/minuto
    throw new Error("Rate limit exceeded");
  }

  limit.count++;
  return true;
}
```

---

## ğŸ“ Conceptos Avanzados

### Chain of Thought (Cadena de Pensamiento):

```typescript
// Hacer que el LLM "piense en voz alta"
const thoughtPrompt = `
Piensa paso a paso para responder:

Usuario pregunta: "${userMessage}"

Paso 1: Â¿QuÃ© informaciÃ³n necesito?
Paso 2: Â¿CÃ³mo puedo estructurar la respuesta?
Paso 3: Â¿Hay algo que deba aclarar?

Ahora responde al usuario:
`;

const response = await generateChatCompletion([
  { role: "system", content: thoughtPrompt },
  { role: "user", content: userMessage },
]);
```

### Few-Shot Learning (Ejemplos):

```typescript
const systemPrompt = `
Eres WADI, un asistente amigable. Ejemplos:

Usuario: "Hola"
WADI: "Â¡Hola! Â¿En quÃ© puedo ayudarte hoy?"

Usuario: "Â¿QuÃ© haces?"
WADI: "Soy un asistente de IA. Puedo ayudarte con preguntas, crear proyectos, buscar informaciÃ³n y mÃ¡s. Â¿QuÃ© necesitas?"

Usuario: "AdiÃ³s"
WADI: "Â¡Hasta luego! Que tengas un excelente dÃ­a."

Ahora responde al usuario:
`;
```

### Temperature Control (Control de Creatividad):

```typescript
// Para respuestas mÃ¡s predecibles (FAQ, datos)
temperature: 0.3;

// Para conversaciÃ³n natural
temperature: 0.7;

// Para respuestas creativas (historias, ideas)
temperature: 0.9;
```

---

## ğŸ”¬ Debugging Avanzado

### Console Logs Estructurados:

```typescript
const DEBUG = process.env.DEBUG === "true";

function debugLog(category: string, ...args: any[]) {
  if (DEBUG) {
    const timestamp = new Date().toISOString();
    console.log(`[${timestamp}] [${category}]`, ...args);
  }
}

// Uso:
debugLog("KIVO", "Analyzing input:", input);
debugLog("WADI", "Executing action:", action);
debugLog("OPENAI", "Response received:", response);
```

### Request Tracing:

```typescript
// Generar ID Ãºnico para cada request
import { v4 as uuid } from "uuid";

function generateTraceId() {
  return uuid();
}

// En cada endpoint:
const traceId = generateTraceId();
req.traceId = traceId;

console.log(`[${traceId}] Request started`);
console.log(`[${traceId}] Kivo analysis complete`);
console.log(`[${traceId}] OpenAI call initiated`);
console.log(`[${traceId}] Response sent`);
```

---

## ğŸ“ˆ Benchmarks

### Tiempos Esperados:

```
Kivo (anÃ¡lisis):       ~10ms (heurÃ­stico)
Kivo (LLM-based):      ~500ms (futuro)
Wadi (ejecuciÃ³n):      ~5ms (routing)
OpenAI API:            ~1000-3000ms (depende del prompt)
Total (guest mode):    ~1000-3500ms
```

### OptimizaciÃ³n de Costos:

```
Modelo: gpt-3.5-turbo
Precio: $0.0005 / 1K tokens (input)
        $0.0015 / 1K tokens (output)

Mensaje promedio: ~200 tokens total
Costo por mensaje: ~$0.0004
1000 mensajes: ~$0.40

GPT-4 serÃ­a:
1000 mensajes: ~$6.00 (15x mÃ¡s caro)
```

---

Esta es la arquitectura completa del cerebro de WADI. Â¿Quieres que profundice en algÃºn aspecto especÃ­fico?
